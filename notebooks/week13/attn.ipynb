{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d9d386",
   "metadata": {},
   "source": [
    "## Attention and Self-Attention\n",
    "\n",
    "Suppose we have a set of $n$ information vectors (the \"memory\"):\n",
    "\n",
    "$$\n",
    "V_1, V_2, \\dots, V_n \\in \\mathbb{R}^d.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd583d0",
   "metadata": {},
   "source": [
    "\n",
    "We want to produce a single output vector that is a weighted combination of these:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\sum_{i=1}^n \\alpha_i V_i,\n",
    "$$\n",
    "\n",
    "where the weights $\\alpha_i$ satisfy:\n",
    "\n",
    "- $\\alpha_i \\ge 0$ for all $i$,\n",
    "- $\\sum_{i=1}^n \\alpha_i = 1$.\n",
    "\n",
    "i.e. **probability distribution** over the vectors $V_i$ telling us how much to \"attend\" to each one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c70e4",
   "metadata": {},
   "source": [
    "## Why? \n",
    "\n",
    "Attention answers:\n",
    "\n",
    "1. How similar is each memory vector $V_i$ to what we are looking for?\n",
    "2. How do we convert these similarities into normalized weights $\\alpha_i$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c44db",
   "metadata": {},
   "source": [
    "## Idea of attention\n",
    "\n",
    "- **query vector** $Q$: \"what we are looking for\"\n",
    "- **key vector** $K_i$ (for matching) and a **value vector** $V_i$ (for content).\n",
    "- attention weights from **queryâ€“key similarity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d0d87",
   "metadata": {},
   "source": [
    "## In math\n",
    "\n",
    "Each item in memory has:\n",
    "\n",
    "- A **key** $K_i \\in \\mathbb{R}^d$, used to decide *whether* we should attend to it.\n",
    "- A **value** $V_i \\in \\mathbb{R}^d$, used as the actual content we mix together.\n",
    "\n",
    "We also have a **query** vector $Q \\in \\mathbb{R}^d$.\n",
    "\n",
    "1. Compute a score for each item $i$:\n",
    "\n",
    "$$\n",
    "s_i = \\langle Q, K_i \\rangle\n",
    "$$\n",
    "\n",
    "(often a dot product).\n",
    "\n",
    "2. Normalize scores with a softmax:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^n \\exp(s_j)}.\n",
    "$$\n",
    "\n",
    "3. Combine values:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\sum_{i=1}^n \\alpha_i V_i.\n",
    "$$\n",
    "\n",
    "In vectorized form, if we stack keys into a matrix $K \\in \\mathbb{R}^{n \\times d}$ and values into $V \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "- Scores: $s = K Q^\\top \\in \\mathbb{R}^n$,\n",
    "- Weights: $\\alpha = \\text{softmax}(s) \\in \\mathbb{R}^n$,\n",
    "- Output: $\\text{Attention}(Q,K,V) = \\alpha^\\top V \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966917b7",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8580aa3",
   "metadata": {},
   "source": [
    "## Self-attention\n",
    "\n",
    "- In standard attention, $Q$, $K$, and $V$ may come from **different** sequences.  \n",
    "- In **self-attention**, they all come from the **same** sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333517fa",
   "metadata": {},
   "source": [
    "## In linear algebra\n",
    "\n",
    "$$\n",
    "  Q = X W_Q, \\quad\n",
    "  K = X W_K, \\quad\n",
    "  V = X W_V, \\quad\n",
    "  \\text{Output} = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V.\n",
    "$$\n",
    "\n",
    "The $W_{\\{Q,K,V\\}}$ are learnable projection matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20663a6f",
   "metadata": {},
   "source": [
    "## Multi-head self attention\n",
    "\n",
    "= stack a bunch of these together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52385fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1550e+00,  1.3382e+00,  1.6987e-03, -1.2204e+00,  3.5535e-01],\n",
       "        [-1.1931e+00,  9.6666e-01,  3.7223e-01,  2.2102e-01,  1.0763e+00],\n",
       "        [ 9.9946e-02, -1.7015e-01, -1.2487e+00,  7.5870e-01, -4.2486e-01],\n",
       "        [ 1.1354e+00,  1.1884e+00, -1.7155e+00,  5.7872e-01,  9.4685e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In code\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.randn(4, 5)   # shape: (seq_len, d_model)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b99e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q: (d_k,)\n",
    "    K: (n, d_k)\n",
    "    V: (n, d_v)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = (K @ Q) / d_k**0.5          # (n,)\n",
    "    weights = F.softmax(scores, dim=0)   # (n,)\n",
    "    output = weights @ V                 # (d_v,)\n",
    "    return output, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8bd4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([ 0.9203,  1.2058, -0.4342, -0.5913,  0.5169])\n",
      "weights: tensor([0.6394, 0.0777, 0.0450, 0.2378])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: use X as both keys and values; choose one token as query\n",
    "Q = X[0]\n",
    "K = X\n",
    "V = X\n",
    "\n",
    "out, w = attention(Q, K, V)\n",
    "print(\"output:\", out)\n",
    "print(\"weights:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "918b4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([4, 5])\n",
      "weights shape: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "def self_attention(X):\n",
    "    \"\"\"\n",
    "    X: (L, d_model)\n",
    "    returns: (L, d_model)\n",
    "    \"\"\"\n",
    "    Q = X        # (L, d_model)\n",
    "    K = X        # (L, d_model)\n",
    "    V = X        # (L, d_model)\n",
    "    \n",
    "    d_k = X.shape[-1]\n",
    "    # scores: (L, L)\n",
    "    scores = Q @ K.T / d_k**0.5  \n",
    "    \n",
    "    # weights: (L, L), row-softmax\n",
    "    weights = F.softmax(scores, dim=1)\n",
    "    \n",
    "    # output: (L, d_model)\n",
    "    output = weights @ V\n",
    "    return output, weights\n",
    "\n",
    "out, w = self_attention(X)\n",
    "print(\"output shape:\", out.shape)\n",
    "print(\"weights shape:\", w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fefb854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1983,  0.2171,  0.2833,  0.2332],\n",
       "        [-0.1736,  0.1477,  0.2335,  0.4675],\n",
       "        [-0.1423,  0.1043,  0.2284,  0.6015],\n",
       "        [-0.1806,  0.1563,  0.2324,  0.4391]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = self.W_Q(X)     # (L, d_k)\n",
    "        K = self.W_K(X)     # (L, d_k)\n",
    "        V = self.W_V(X)     # (L, d_k)\n",
    "\n",
    "        d_k = Q.shape[-1]\n",
    "        scores = Q @ K.T / d_k**0.5\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        return weights @ V\n",
    "\n",
    "sa = SelfAttention(d_model=5, d_k=4)\n",
    "out = sa(X)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e42b9aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0437, -0.0520,  0.3056,  0.2199,  0.0880, -0.1595,  0.0579,  0.0900],\n",
       "        [-0.0722,  0.1792,  0.4519,  0.1704,  0.0363, -0.0569, -0.0437,  0.1474],\n",
       "        [-0.0450, -0.0899,  0.2666,  0.2517,  0.1145, -0.1877,  0.0941,  0.1019],\n",
       "        [-0.0671, -0.0384,  0.2782,  0.2572,  0.1151, -0.1889,  0.0960,  0.1342]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_O = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        L, d_model = X.shape\n",
    "\n",
    "        Q = self.W_Q(X)     # (L, d_model)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "\n",
    "        # reshape into heads\n",
    "        Q = Q.view(L, self.num_heads, self.d_head).transpose(0,1)  # (h, L, d_head)\n",
    "        K = K.view(L, self.num_heads, self.d_head).transpose(0,1)\n",
    "        V = V.view(L, self.num_heads, self.d_head).transpose(0,1)\n",
    "\n",
    "        scores = Q @ K.transpose(1,2) / (self.d_head ** 0.5)       # (h, L, L)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        out = weights @ V                                          # (h, L, d_head)\n",
    "\n",
    "        # merge heads\n",
    "        out = out.transpose(0,1).contiguous().view(L, d_model)\n",
    "        return self.W_O(out)\n",
    "\n",
    "mhsa = MultiHeadSelfAttention(d_model=8, num_heads=2)\n",
    "X2 = torch.randn(4, 8)\n",
    "mhsa(X2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
